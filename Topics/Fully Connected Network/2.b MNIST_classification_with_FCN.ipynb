{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section now builds and trains a FCN on the MNIST dataset. Since there are 10 handwritten digit labels, this becomes a multi-class classification task. In terms of the architecture, this is as simple as changing the output dimension.\\\n",
    "However, the evaluation metrics involves a bit more consideration. In the previous section, we used Cross Entropy Loss. This is still applicable in the multi-class case. However, other commonly used metrics for binary classification like Logistic Loss will not be applicable, or may give wrong interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_available=True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Grayscale\n",
    "\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f'{gpu_available=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from CustomDL.utils import train_model, test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "digits_dataset = MNIST(\n",
    "    root='../../data', # the folder where data eixsts/is stored,\n",
    "    download=True,\n",
    "    transform=ToTensor() # The image are PIL format, this transform to Tensor objects\n",
    ")\n",
    "print(digits_dataset.data.shape)\n",
    "\n",
    "train_data, test_data = random_split(digits_dataset, [.8, .2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneClassfier(nn.Module):\n",
    "    def __init__(self,\n",
    "        hidden_dims: int,\n",
    "        activation_fn: nn.Module\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, hidden_dims),\n",
    "            activation_fn,\n",
    "            nn.Linear(hidden_dims, hidden_dims),\n",
    "            activation_fn,\n",
    "            nn.Linear(hidden_dims, hidden_dims),\n",
    "            activation_fn,\n",
    "            nn.Linear(hidden_dims, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.layers(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneClassfier(100, nn.ReLU())\n",
    "if gpu_available:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs(\n",
    "    epochs: int,\n",
    "    model: nn.Module,\n",
    "    train_loader: tuple[DataLoader, DataLoader],\n",
    "    test_loader: tuple[DataLoader, DataLoader],\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: nn.Module\n",
    "):\n",
    "    num_dig = int(math.log10(epochs)) + 1\n",
    "    update_rate = 1 if epochs <= 20 else 10\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if not epoch % update_rate:\n",
    "            print(f\"Epoch {epoch + 1:>{num_dig}}/{epochs}\", end=' || ')\n",
    "        train_model(model, train_loader, loss_fn, optimizer,\n",
    "                    use_gpu=gpu_available)\n",
    "        loss, acc = test_model(model, test_loader, loss_fn, True,\n",
    "                               use_gpu=gpu_available)\n",
    "        if not epoch % update_rate:\n",
    "            print(f\"Average Loss: {loss:.6f} | {acc * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size, shuffle=True)\n",
    "\n",
    "sml = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/20 || Average Loss: 2.302891 | 8.9833%\n",
      "Epoch  2/20 || Average Loss: 2.297696 | 8.9833%\n",
      "Epoch  3/20 || Average Loss: 2.292203 | 8.9833%\n",
      "Epoch  4/20 || Average Loss: 2.285771 | 9.0667%\n",
      "Epoch  5/20 || Average Loss: 2.277879 | 15.4583%\n",
      "Epoch  6/20 || Average Loss: 2.267418 | 22.1917%\n",
      "Epoch  7/20 || Average Loss: 2.253236 | 27.5750%\n",
      "Epoch  8/20 || Average Loss: 2.233504 | 30.6083%\n",
      "Epoch  9/20 || Average Loss: 2.204786 | 33.8000%\n",
      "Epoch 10/20 || Average Loss: 2.161072 | 36.9917%\n",
      "Epoch 11/20 || Average Loss: 2.092691 | 39.9083%\n",
      "Epoch 12/20 || Average Loss: 1.987840 | 43.8667%\n",
      "Epoch 13/20 || Average Loss: 1.837259 | 49.3833%\n",
      "Epoch 14/20 || Average Loss: 1.640134 | 57.5417%\n",
      "Epoch 15/20 || Average Loss: 1.414235 | 65.2250%\n",
      "Epoch 16/20 || Average Loss: 1.204940 | 69.9417%\n",
      "Epoch 17/20 || Average Loss: 1.041373 | 72.5250%\n",
      "Epoch 18/20 || Average Loss: 0.920221 | 74.6333%\n",
      "Epoch 19/20 || Average Loss: 0.830743 | 76.5083%\n",
      "Epoch 20/20 || Average Loss: 0.764431 | 78.0417%\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "run_epochs(\n",
    "    epochs, model,\n",
    "    train_loader, test_loader,\n",
    "    sml, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {'epochs': epochs, 'hidden_dims': 100, 'model': model.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "    './output/digits_classifier_1.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneClassfier(\n",
       "  (layers): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=100, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (6): ReLU()\n",
       "    (7): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21d97c25310>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_img = torchvision.io.read_image('./data/num2.png')\n",
    "sample_img = Grayscale(1)(sample_img[:3])\n",
    "sample_img = (255 - sample_img) / 255\n",
    "plt.imshow(sample_img[0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output values:\n",
      "    tensor([[ 1.5933, -5.7504, -0.3954,  0.4799,  1.0465,  2.3509,  0.1406,  1.0806,\n",
      "          1.5156,  1.2748]])\n",
      "Class 'Probabilities':\n",
      "    tensor([[1.5008e-01, 9.7051e-05, 2.0541e-02, 4.9288e-02, 8.6858e-02, 3.2014e-01,\n",
      "         3.5109e-02, 8.9877e-02, 1.3886e-01, 1.0914e-01]])\n",
      "Prediction: 5\n"
     ]
    }
   ],
   "source": [
    "pred_vals = model(sample_img.unsqueeze(0)).detach()\n",
    "pred_probs = nn.Softmax(1)(pred_vals)\n",
    "prediction = pred_vals.argmax().item()\n",
    "\n",
    "print(f\"\"\"Output values:\n",
    "    {pred_vals}\n",
    "Class 'Probabilities':\n",
    "    {pred_probs}\n",
    "Prediction: {prediction}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "- Given that images of same class have fairly homogenous form, the model have no problem fitting into the data. This is as expected.\n",
    "- However, the model can only predict a images of fixed size 28x28. Thus, if we are required to do inference on images of different sizes, we will need to manually preprocess those images or maybe use a different architecture that can do this prepocessing automatically for us.\n",
    "- Furthermore, if we increase the image size, we need more weights and biases to optimize. We can think of these weights and biases as elements in the netwok that help differentiate between one number and another. If we have too many criteria to differentiate numbers, it will create unnecessary complexity and thus prone to overfitting.\n",
    "\n",
    "For the last point, we can avoid such an issue by finding a way to extract some of the more prominent features inside an image, and then pass those extracted features into the network. We also notice that different numebrs have subtle relationships between the neighbouring pixels. We can pick up these relationships by finding a suitable filter to apply over the image. This then leads to the idea of convolving the image with a fixed kernel that have the appropriate weights.\\\n",
    "When we integrate this convolution operation within the network, one obtains a Convolutional Neural Network. This architecture is explored further in the Convoluntional Network topic. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
