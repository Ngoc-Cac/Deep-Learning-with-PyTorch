{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that using CNM, we can capture certain information about neighbouring pixels, something that a neural network with only fully connected layer can't do very well.\\\n",
    "However, since we also use a fully connected layer at the end of the convolutional layer to fit the encoded images and classify them, the CNN we defined is also subject to a fixed size input. One way to approach this problem is to transform the image into a size the network can take. For example, through cropping, resizing, downsampling the image. We demonstrated this by applying a low-pass filter, that is, using a Gaussian blur on the image to smooth out features and then downsample the image using a constant hop size.\\\n",
    "Here, we demonstrate another way to deal with the problem of fixed size input.\n",
    "\n",
    "We notice that when we do a convolution on an image, we need to know three things: the kernel weights (which implies the size as well), the number of input filters and the number of output filters. However, none of this is dependent on the image size. This then implies that if our network only consists of convolutional layers (which includes the pooling layers as well), our network would not need to know the image size beforehand and can apply the convolutions on any images as long as the channel dimension matches.\\\n",
    "This brings up the idea of a fully convolutional network where we completely get rid of the final fully connected layers and replace them with convolutional layers. This section defines a simple FCNN that is exactly equivalent to the CNN defined in section 1.a with a minor enhancement that this network can now recieve a grayscale-valued image of any arbitrary shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_available=True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f'{gpu_available=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from CustomDL.loops.classification import train_loop, test_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "digits_dataset = MNIST(\n",
    "    root='../../data',\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "print(digits_dataset.data.shape)\n",
    "\n",
    "train_data, test_data = random_split(digits_dataset, [.8, .2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Fully Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_chans: int,\n",
    "        out_chans: int,\n",
    "        kern_size: int,\n",
    "        activation_fn: Optional[nn.Module] = None,\n",
    "        pooling_layer: Optional[nn.Module] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_chans,\n",
    "                out_channels=out_chans,\n",
    "                kernel_size=kern_size\n",
    "            )\n",
    "        )\n",
    "        if not activation_fn is None:\n",
    "            self.layers.add_module('1', activation_fn)\n",
    "        if not pooling_layer is None:\n",
    "            self.layers.add_module('2', pooling_layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dim = (in_dim - (kern_size - 1) - 1) / stride + 1\n",
    "fcnn = nn.Sequential(\n",
    "    ConvLayer(1, 4, 3, nn.ReLU(), nn.MaxPool2d(2)),\n",
    "    ConvLayer(4, 9, 3, nn.ReLU(), nn.MaxPool2d(2)),\n",
    "\n",
    "    ConvLayer(9, 100, 5, nn.ReLU()),\n",
    "    ConvLayer(100, 100, 1, nn.ReLU()),\n",
    "    ConvLayer(100, 10, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we defined the FCNN to have five convolutional layers. However, the last three layers actually represents the fully connected layers that we had in the older CNN network. To see why, we compute the shape of the input layer-by-layer:\n",
    "\n",
    "**ConvLayer (1)**\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "The original image has the shape (1, 28, 28). We then apply a convolution with a (3, 3) filter and stride of 1. Following the formula given in the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d):\n",
    "$$\n",
    "\\text{out} = \\frac{\\text{in}-(3 - 1)-1}{1}+1=28-2=26\n",
    "$$\n",
    "which gives us an image of shape (4, 26, 26). Finally, we apply max pooling using a (2, 2) window and stride of 2. The resulting shape can be obtained by dividing the original shape by 2 and take the floor of the division.\\\n",
    "Finally, after the convolution and pooling layers, we obtain an image of shape (4, 13, 13).\n",
    "\n",
    "**ConvLayer (2)**\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Similar to the layer above, this layer take in a 4-channel image of arbitrary size (13x13 using the result above). Then, it applies a convolution with a (3, 3) filter and stride of 1, which results in a (9, 11, 11) image. After max pooling with a (2, 2) window and stride of 2, we get a (9, 5, 5) image.\\\n",
    "\n",
    "**ConvLayer (3)**\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "In terms of the old CNN, this is where we flatten the output of the previous convolutional layer to pass it into a fully connected layers of 100 perceptrons. Mathematically speaking, we obtained a value for a perceptron in the fully connected layer by multiplying the (1, $4\\cdot5\\cdot5$) pixel values by some (100, 1) weight matrix. After which, we added the result to some bias.\\\n",
    "So in terms of the (9, 5, 5) image, we multiplied each (5, 5) channel in the image by some (5, 5) weight matrix element-wise and added them together respectively. After which, we added the result with some bias. This exactly represents the convolution process on the (5, 5) channel with a (5, 5) filter and **no padding**. If we repeat this 100 times, we get a convolution with output channels of 100.\\\n",
    "We can see this in the code above by defining a ConvLayer with input channels of 9, output channels of 100 and a kernel size of 5. We also note that we do not apply any max pooling.\n",
    "\n",
    "**ConvLayer (4)** and **ConvLayer (5)**\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Likewise, a fully connected layer of 100 inputs into 100 perceptrons is equivalent to a convolution on the 100-channel 'image' of size (1, 1) to output another 100-channel 'image' of size (1, 1). Finally, we define the output layer to be a convolutional layer with 100 input channels and 10 output channels. We can interpret this result the same way as we interpret the result of the old CNN.\\\n",
    "However, we need to note that we are receiving an output vector of shape (..., 10, 1, 1) and not the regular (..., 1, 10).\n",
    "\n",
    "---\n",
    "\n",
    "Note: in order for us to have the output of (10, 1, 1) for the final convolutional layer, we had to compute the output shape of the second ConvLayer. This is something that we also had to do for the other CNN. A question raised is why we would use this in place of the CNN if we have to do the same thing. This is discussed later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 1, 1])\n",
      "tensor(2.2802)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.2802)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_image = digits_dataset.data[:2].unsqueeze(1) / 255\n",
    "sample_labels = digits_dataset.targets[:2]\n",
    "with torch.no_grad():\n",
    "    forward_res = fcnn(sample_image)\n",
    "\n",
    "# Note that during training, we should also expect (batch, 10, 1, 1) outputs.\n",
    "print(forward_res.shape)\n",
    "\n",
    "# however, most loss functions, like CrossEntropyLoss expects a shape (batch, C) for target shape (C)\n",
    "# so we need to 'squeeze out' the extra dimensions or we get an error.\n",
    "out = nn.CrossEntropyLoss()(forward_res.squeeze(), sample_labels)\n",
    "print(out)\n",
    "\n",
    "# However, note that if batch size is 1, forward_res.squeeze() can return shape (10) which\n",
    "# will also throw an error. We can use flatten as an alternative\n",
    "nn.CrossEntropyLoss()(torch.flatten(forward_res, 1), sample_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either change the train loop to do the squeeze operation, or do it manually in the forward method.\\\n",
    "Thus, we define a class for the FCNN. Note that this should only be done during training. When deploying the model, we do not flatten the output anymore because that is the nature of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            ConvLayer(1, 4, 3, nn.ReLU(), nn.MaxPool2d(2)),\n",
    "            ConvLayer(4, 9, 3, nn.ReLU(), nn.MaxPool2d(2)),\n",
    "\n",
    "            ConvLayer(9, 100, 5, nn.ReLU()),\n",
    "            ConvLayer(100, 100, 1, nn.ReLU()),\n",
    "            ConvLayer(100, 10, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, one_chan_image):\n",
    "        res = self.layers(one_chan_image)\n",
    "        return torch.flatten(res, 1) # convenience for computing loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs(\n",
    "    epochs: int,\n",
    "    model: nn.Module,\n",
    "    train_loader: tuple[DataLoader, DataLoader],\n",
    "    test_loader: tuple[DataLoader, DataLoader],\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: nn.Module\n",
    "):\n",
    "    num_dig = int(math.log10(epochs)) + 1\n",
    "    update_rate = 1 if epochs <= 20 else 10\n",
    "    loss, acc = None, None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1:>{num_dig}}/{epochs}\")\n",
    "        loss = train_loop(model, train_loader, loss_fn, optimizer,\n",
    "                          use_gpu=gpu_available)\n",
    "        print(f\"  Average Training Loss: {sum(loss) / len(loss):.6f}\")\n",
    "\n",
    "        loss, acc = test_loop(model, test_loader, loss_fn, True,\n",
    "                               use_gpu=gpu_available)\n",
    "        print(f\"  Average Eval Loss: {loss:.6f} | {acc * 100:.4f}%\")\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_data, shuffle=True)\n",
    "\n",
    "fcnn = FCNN()\n",
    "if gpu_available: fcnn.cuda()\n",
    "\n",
    "cross_entrop = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(fcnn.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/20 || Average Loss: 2.300262 | 13.8333%\n",
      "Epoch  2/20 || Average Loss: 2.296425 | 11.9000%\n",
      "Epoch  3/20 || Average Loss: 2.291495 | 12.5917%\n",
      "Epoch  4/20 || Average Loss: 2.283396 | 18.5167%\n",
      "Epoch  5/20 || Average Loss: 2.267552 | 28.9167%\n",
      "Epoch  6/20 || Average Loss: 2.225723 | 42.9500%\n",
      "Epoch  7/20 || Average Loss: 2.017937 | 52.6667%\n",
      "Epoch  8/20 || Average Loss: 1.054533 | 70.3083%\n",
      "Epoch  9/20 || Average Loss: 0.643230 | 80.6000%\n",
      "Epoch 10/20 || Average Loss: 0.507024 | 84.7583%\n",
      "Epoch 11/20 || Average Loss: 0.438814 | 87.0250%\n",
      "Epoch 12/20 || Average Loss: 0.383199 | 88.4750%\n",
      "Epoch 13/20 || Average Loss: 0.348915 | 89.4667%\n",
      "Epoch 14/20 || Average Loss: 0.327089 | 89.9250%\n",
      "Epoch 15/20 || Average Loss: 0.305629 | 90.5417%\n",
      "Epoch 16/20 || Average Loss: 0.284852 | 91.2333%\n",
      "Epoch 17/20 || Average Loss: 0.270700 | 91.5583%\n",
      "Epoch 18/20 || Average Loss: 0.255782 | 92.1333%\n",
      "Epoch 19/20 || Average Loss: 0.241370 | 92.4750%\n",
      "Epoch 20/20 || Average Loss: 0.228948 | 93.0833%\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "loss, acc = run_epochs(\n",
    "    epochs, fcnn,\n",
    "    trainloader, testloader,\n",
    "    cross_entrop,\n",
    "    optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'train_params': {'batch_size': batch_size, 'lr': learning_rate, 'epochs': epochs},\n",
    "    'loss': loss,\n",
    "    'accuracy': acc,\n",
    "    'model': fcnn.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}\n",
    "torch.save(params, './output/fcnn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FCNN can converge as well as the CNN can. Again, this is because the FCNN that we defined is equivalent to that of the CNN but we just replaced the Fully Connected Layer with a Convolutional Layer that does not apply max pooling.\n",
    "\n",
    "So from this, we know that the CNN we saw in section 1.a and 1.b can be constructed purely from just convolution operations with slightly slower running time. Next section will be going into how these convolution operations can help us use the network on images of different sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
